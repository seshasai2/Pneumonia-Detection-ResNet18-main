{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, time\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# ---------------- USER CONFIG ----------------\nDATA_DIR = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"   # change as needed\nOUT_DIR = Path(\"fast_outputs\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\nQUICK = True               # set False to run full dataset\nSUBSAMPLE_FRAC = 0.20      # when QUICK=True, use 20% of training data\nIMG_SIZE = 160             # smaller -> faster; set 224 for final runs\nBATCH_SIZE = 32\nBASE_EPOCHS = 2            # quick baseline epochs\nFINETUNE_EPOCHS = 3        # quick fine-tune epochs\nLR_BASE = 1e-3\nLR_FINETUNE = 2e-4\nNUM_WORKERS = 2            # keep small for Kaggle/Colab stability\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n# ----------------------------------------------\n\nprint(\"Device:\", DEVICE)\ntrain_dir = os.path.join(DATA_DIR, \"train\")\nval_dir = os.path.join(DATA_DIR, \"val\")\ntest_dir = os.path.join(DATA_DIR, \"test\")\nfor p in (train_dir, val_dir, test_dir):\n    if not os.path.exists(p):\n        raise FileNotFoundError(f\"Missing folder: {p}\")\n\n# --------- transforms & datasets ----------\ntrain_tf_baseline = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\ntrain_tf_finetune = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.08,0.08,0.08,0.02),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\ntest_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\ntrain_ds_full = datasets.ImageFolder(train_dir, transform=train_tf_baseline)\nval_ds = datasets.ImageFolder(val_dir, transform=test_tf)\ntest_ds = datasets.ImageFolder(test_dir, transform=test_tf)\n\n# optional quick subsample to speed up experiments\nif QUICK:\n    n_sub = max(200, int(len(train_ds_full) * SUBSAMPLE_FRAC))\n    idxs = sorted(random.sample(range(len(train_ds_full)), n_sub))\n    train_ds = Subset(train_ds_full, idxs)\n    print(f\"QUICK mode: using {n_sub} / {len(train_ds_full)} training samples\")\nelse:\n    train_ds = train_ds_full\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n\nclass_names = train_ds_full.classes\nprint(\"Classes:\", class_names)\nprint(\"Sizes (train/val/test):\", len(train_ds), len(val_ds), len(test_ds))\n\n# -------- helpers ----------\ndef train_epoch(model, loader, optimizer, criterion, device, scaler=None, use_amp=False):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for xb, yb in loader:\n        xb, yb = xb.to(device), yb.to(device)\n        optimizer.zero_grad()\n        if use_amp and scaler is not None:\n            with torch.cuda.amp.autocast():\n                out = model(xb)\n                loss = criterion(out, yb)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            out = model(xb)\n            loss = criterion(out, yb)\n            loss.backward()\n            optimizer.step()\n        running_loss += float(loss.item()) * xb.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == yb).sum().item()\n        total += xb.size(0)\n    return running_loss/total, correct/total\n\ndef eval_model(model, loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    preds_all, labels_all = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = criterion(out, yb)\n            running_loss += float(loss.item()) * xb.size(0)\n            preds = out.argmax(dim=1)\n            preds_all.extend(preds.cpu().numpy())\n            labels_all.extend(yb.cpu().numpy())\n            correct += (preds == yb).sum().item()\n            total += xb.size(0)\n    return running_loss/total, correct/total, np.array(preds_all), np.array(labels_all)\n\n# ---------------- BASELINE (ResNet18 frozen) ----------------\nprint(\"\\n== Baseline (ResNet18 frozen backbone) ==\")\nmodel_base = models.resnet18(pretrained=True)\nfor p in model_base.parameters(): p.requires_grad = False\nnum_ftrs = model_base.fc.in_features\nmodel_base.fc = nn.Linear(num_ftrs, len(class_names))\nmodel_base = model_base.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_base.fc.parameters(), lr=LR_BASE)\n\nbest_val_acc = 0.0\nbest_base_path = OUT_DIR/\"best_resnet18.pt\"\n\nfor epoch in range(BASE_EPOCHS):\n    t0 = time.time()\n    train_loss, train_acc = train_epoch(model_base, train_loader, optimizer, criterion, DEVICE)\n    val_loss, val_acc, _, _ = eval_model(model_base, val_loader, criterion, DEVICE)\n    print(f\"Baseline Epoch {epoch+1}/{BASE_EPOCHS} train_acc:{train_acc:.4f} val_acc:{val_acc:.4f} time:{time.time()-t0:.1f}s\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model_base.state_dict(), best_base_path)\n        print(\"Saved baseline model:\", best_base_path)\n\n# baseline test evaluation\nmodel_base.load_state_dict(torch.load(best_base_path, map_location=DEVICE))\n_, test_acc_base, preds_b, labels_b = eval_model(model_base, test_loader, criterion, DEVICE)\nprint(\"Baseline test acc:\", test_acc_base)\nprint(classification_report(labels_b, preds_b, target_names=class_names))\ncm = confusion_matrix(labels_b, preds_b)\nplt.figure(figsize=(4,3)); sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names); plt.title(\"Baseline Confusion\"); plt.savefig(OUT_DIR/\"confusion_baseline.png\"); plt.close()\n\n# -------- Quick Grad-CAM for baseline ----------\ndef gradcam_resnet_simple(model, tensor_img, target_class, layer_name=\"layer4\"):\n    model.eval()\n    features, gradients = [], []\n    def forward_hook(m, i, o): features.append(o)\n    def backward_hook(m, gi, go): gradients.append(go[0])\n    module = getattr(model, layer_name)\n    fh = module.register_forward_hook(forward_hook)\n    bh = module.register_backward_hook(backward_hook)\n    out = model(tensor_img)\n    score = out[0, target_class]\n    model.zero_grad()\n    score.backward(retain_graph=True)\n    if not features or not gradients:\n        fh.remove(); bh.remove(); raise RuntimeError(\"Failed hooks\")\n    act = features[-1][0].detach(); grad = gradients[-1][0].detach()\n    weights = grad.mean(dim=(1,2))\n    cam = (weights.view(-1,1,1) * act).sum(dim=0).cpu().numpy()\n    cam = np.maximum(cam, 0); cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n    if cam.max()>0: cam = (cam - cam.min())/(cam.max()+1e-8)\n    fh.remove(); bh.remove()\n    return cam\n\nprint(\"Generating 4 baseline Grad-CAM images...\")\nfor i in range(4):\n    img, lbl = test_ds[i]\n    inp = img.unsqueeze(0).to(DEVICE)\n    try:\n        cam = gradcam_resnet_simple(model_base, inp, target_class=lbl)\n    except Exception as e:\n        print(\"Grad-CAM err:\", e); break\n    img_np = img.permute(1,2,0).cpu().numpy(); img_np = (img_np * np.array([0.229,0.224,0.225])) + np.array([0.485,0.456,0.406])\n    fname = OUT_DIR/f\"gradcam_base_{i}_pred{lbl}.png\"\n    plt.figure(figsize=(4,4)); plt.imshow(img_np); plt.imshow(cam, cmap='jet', alpha=0.4); plt.axis('off'); plt.savefig(fname); plt.close()\nprint(\"Saved baseline Grad-CAMs to\", OUT_DIR)\n\n# ---------------- Fine-tune (small, mixed precision) ----------------\nprint(\"\\n== Fine-tune (unfreeze last block) ==\")\n# Use same model architecture (ResNet18) for speed; unfreeze layer4 + fc\nmodel_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))\n# unfreeze last block and fc\nfor name, param in model_ft.named_parameters():\n    if \"layer4\" in name or \"fc\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\nmodel_ft = model_ft.to(DEVICE)\n\nparams_to_opt = [p for p in model_ft.parameters() if p.requires_grad]\noptimizer_ft = optim.AdamW(params_to_opt, lr=LR_FINETUNE, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=FINETUNE_EPOCHS)\nscaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n\nbest_val_acc = 0.0\nbest_ft_path = OUT_DIR/\"best_finetuned_resnet18.pt\"\n\n# use a DataLoader with stronger augment (train_tf_finetune) but reuse dataset indices\ntrain_ds_finetune_full = datasets.ImageFolder(train_dir, transform=train_tf_finetune)\nif QUICK:\n    train_ds_finetune = Subset(train_ds_finetune_full, idxs)  # same idxs as before\nelse:\n    train_ds_finetune = train_ds_finetune_full\ntrain_loader_ft = DataLoader(train_ds_finetune, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n\nfor epoch in range(FINETUNE_EPOCHS):\n    t0 = time.time()\n    # training with mixed precision\n    model_ft.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for xb, yb in train_loader_ft:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        optimizer_ft.zero_grad()\n        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n            out = model_ft(xb)\n            loss = nn.CrossEntropyLoss()(out, yb)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer_ft)\n        scaler.update()\n        running_loss += float(loss.item())*xb.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds==yb).sum().item()\n        total += xb.size(0)\n    scheduler.step()\n    val_loss, val_acc, _, _ = eval_model(model_ft, val_loader, nn.CrossEntropyLoss(), DEVICE)\n    print(f\"Fine-tune Epoch {epoch+1}/{FINETUNE_EPOCHS} train_acc:{(correct/total if total else 0):.4f} val_acc:{val_acc:.4f} time:{time.time()-t0:.1f}s\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model_ft.state_dict(), best_ft_path)\n        print(\"Saved fine-tuned model:\", best_ft_path)\n\n# final eval\nmodel_ft.load_state_dict(torch.load(best_ft_path, map_location=DEVICE))\n_, test_acc_ft, preds_ft, labels_ft = eval_model(model_ft, test_loader, nn.CrossEntropyLoss(), DEVICE)\nprint(\"Fine-tuned test acc:\", test_acc_ft)\nprint(classification_report(labels_ft, preds_ft, target_names=class_names))\ncm = confusion_matrix(labels_ft, preds_ft)\nplt.figure(figsize=(4,3)); sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names); plt.title(\"Finetuned Confusion\"); plt.savefig(OUT_DIR/\"confusion_finetuned.png\"); plt.close()\n\n# Grad-CAM on fine-tuned model (4 samples)\nprint(\"Generating Grad-CAMs for fine-tuned model...\")\nfor i in range(4):\n    img, lbl = test_ds[i]\n    inp = img.unsqueeze(0).to(DEVICE)\n    try:\n        cam = gradcam_resnet_simple(model_ft, inp, target_class=lbl)\n    except Exception as e:\n        print(\"Grad-CAM fine err:\", e); break\n    img_np = img.permute(1,2,0).cpu().numpy(); img_np = (img_np * np.array([0.229,0.224,0.225])) + np.array([0.485,0.456,0.406])\n    fname = OUT_DIR/f\"gradcam_finetune_{i}_pred{lbl}.png\"\n    plt.figure(figsize=(4,4)); plt.imshow(img_np); plt.imshow(cam, cmap='jet', alpha=0.4); plt.axis('off'); plt.savefig(fname); plt.close()\nprint(\"Saved fine-tune Grad-CAMs to\", OUT_DIR)\n\n# Save inference helper\ninference_py = f'''\n# inference_helper.py\nimport torch, torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision import models\nimport numpy as np\n\ndef load_model(path):\n    m = models.resnet18(pretrained=False)\n    m.fc = torch.nn.Linear(m.fc.in_features, {len(class_names)})\n    m.load_state_dict(torch.load(path, map_location='cpu'))\n    m.eval()\n    return m\n\ndef predict(img_path, model_path):\n    tf = transforms.Compose([transforms.Resize(({IMG_SIZE},{IMG_SIZE})), transforms.ToTensor(),\n                              transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n    img = Image.open(img_path).convert('RGB')\n    x = tf(img).unsqueeze(0)\n    m = load_model(model_path)\n    with torch.no_grad():\n        out = m(x)\n        p = torch.softmax(out, dim=1).numpy()[0]\n    return p\n\nif __name__=='__main__':\n    import sys\n    print(predict(sys.argv[2], sys.argv[1]))\n'''\n(OUT_DIR/\"inference_helper.py\").write_text(inference_py)\n\n# final summary\nprint(\"\\n=== DONE ===\")\nprint(\"Artifacts in:\", OUT_DIR)\nprint(\"Baseline model:\", best_base_path)\nprint(\"Fine-tuned model:\", best_ft_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T13:46:37.414303Z","iopub.execute_input":"2025-10-30T13:46:37.414749Z","iopub.status.idle":"2025-10-30T13:51:05.252752Z","shell.execute_reply.started":"2025-10-30T13:46:37.414716Z","shell.execute_reply":"2025-10-30T13:51:05.251577Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nQUICK mode: using 1043 / 5216 training samples\nClasses: ['NORMAL', 'PNEUMONIA']\nSizes (train/val/test): 1043 16 624\n\n== Baseline (ResNet18 frozen backbone) ==\nBaseline Epoch 1/2 train_acc:0.8360 val_acc:0.5000 time:35.8s\nSaved baseline model: fast_outputs/best_resnet18.pt\nBaseline Epoch 2/2 train_acc:0.8993 val_acc:0.6875 time:37.6s\nSaved baseline model: fast_outputs/best_resnet18.pt\nBaseline test acc: 0.7419871794871795\n              precision    recall  f1-score   support\n\n      NORMAL       0.95      0.33      0.49       234\n   PNEUMONIA       0.71      0.99      0.83       390\n\n    accuracy                           0.74       624\n   macro avg       0.83      0.66      0.66       624\nweighted avg       0.80      0.74      0.70       624\n\nGenerating 4 baseline Grad-CAM images...\nGrad-CAM err: Failed hooks\nSaved baseline Grad-CAMs to fast_outputs\n\n== Fine-tune (unfreeze last block) ==\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/tmp/ipykernel_37/4037596576.py:211: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n/tmp/ipykernel_37/4037596576.py:232: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"Fine-tune Epoch 1/3 train_acc:0.9051 val_acc:0.5000 time:49.1s\nSaved fine-tuned model: fast_outputs/best_finetuned_resnet18.pt\nFine-tune Epoch 2/3 train_acc:0.9674 val_acc:0.7500 time:49.1s\nSaved fine-tuned model: fast_outputs/best_finetuned_resnet18.pt\nFine-tune Epoch 3/3 train_acc:0.9847 val_acc:0.8125 time:49.8s\nSaved fine-tuned model: fast_outputs/best_finetuned_resnet18.pt\nFine-tuned test acc: 0.8830128205128205\n              precision    recall  f1-score   support\n\n      NORMAL       0.98      0.71      0.82       234\n   PNEUMONIA       0.85      0.99      0.91       390\n\n    accuracy                           0.88       624\n   macro avg       0.91      0.85      0.87       624\nweighted avg       0.90      0.88      0.88       624\n\nGenerating Grad-CAMs for fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n","output_type":"stream"},{"name":"stdout","text":"Saved fine-tune Grad-CAMs to fast_outputs\n\n=== DONE ===\nArtifacts in: fast_outputs\nBaseline model: fast_outputs/best_resnet18.pt\nFine-tuned model: fast_outputs/best_finetuned_resnet18.pt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}